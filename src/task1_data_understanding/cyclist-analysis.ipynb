{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS (RACERS)\n",
    "In this notebook we are going to asses the quality of the racers dataset provided for the course project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "datasetname=path.join('dataset', 'cyclists.csv')\n",
    "df=pd.read_csv(datasetname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRONG DATA\n",
    "To asses which are the wrong data in the dataset we have to check the data type in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6134 entries, 0 to 6133\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   _url         6134 non-null   object \n",
      " 1   name         6134 non-null   object \n",
      " 2   birth_year   6121 non-null   float64\n",
      " 3   weight       3078 non-null   float64\n",
      " 4   height       3143 non-null   float64\n",
      " 5   nationality  6133 non-null   object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 287.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from the info command we can start to see the first problems, the url, name and nationality should be string types, the birth_year should be an Integer type, weight and height are ok to be float64.\n",
    "Now we can show every element which don't respect the natural type to check what are the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check if the object is an inference choice for string values or if there are some kind of null pointer or not accepted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_type(valore, tipo_atteso):\n",
    "    try:\n",
    "        return not isinstance(valore, tipo_atteso)\n",
    "    except:\n",
    "        return True  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: _url, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "column = '_url'\n",
    "wrong_elements = df[df[column].apply(wrong_type, tipo_atteso=str)]\n",
    "\n",
    "print(wrong_elements[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the elements are str type. Since the url are in the form \"name-surname\" we can check if for all the rows this rule is respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the main differences between the two column is the replacing of the double whitespace with a '-' (name-surname), the replace of the singular withespace with a '-' (first name-second name) and the standardization of the letters with accent in the base form (è in e) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         name                            _url\n",
      "19             Michał  Paluta                   michal-paluta\n",
      "25        Graeme Allen  Brown                    graeme-brown\n",
      "32           Piotr  Przydział                 piotr-przydzial\n",
      "49           Jesús  Rodríguez       jesus-rodriguez-rodriguez\n",
      "68              Ivan  Herrero           iban-herrero-atienzar\n",
      "...                       ...                             ...\n",
      "6093  Raúl Alexander  Montaña  raul-alexander-montana-herrera\n",
      "6112     Juan Carlos  Jusdado      juan-carlos-jusdado-ibanez\n",
      "6113           Joseba  Albizu            joseba-albizu-lizaso\n",
      "6121         Jacob  Hindsgaul          jacob-hindsgaul-madsen\n",
      "6129      Juan José  Martínez         juan-jose-martinez-diaz\n",
      "\n",
      "[618 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "differences = df[df['name'].apply(remove_accent).str.lower().str.replace('  ', '-').str.replace(' ','-') != df['_url']]\n",
    "\n",
    "print(differences[['name', '_url']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis we can see that there are 618 rows which don't respect the main conversion rules, some with missin elements (second name or surname) and some with wrong characters. In some cases in the '_url' there are more information in respect of the 'name' column, because of that it's better to split the column 'name' in 'name' and 'surname' ('secondname' could cause too many empty elements) and rebuild the full name of every rider thanks to the column with more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan indexes '_url': []\n"
     ]
    }
   ],
   "source": [
    "indici_nan = df.index[df[column].isna()].tolist()\n",
    "\n",
    "print(f\"Nan indexes '{column}': {indici_nan}\")\n",
    "print(f\"Numbner of missing values '{column}': {len(indici_nan)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no missing values in this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: name, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "column = 'name'\n",
    "wrong_elements = df[df[column].apply(wrong_type, tipo_atteso=str)]\n",
    "\n",
    "print(wrong_elements[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan indexes 'name': []\n"
     ]
    }
   ],
   "source": [
    "indici_nan = df.index[df[column].isna()].tolist()\n",
    "\n",
    "print(f\"Nan indexes '{column}': {indici_nan}\")\n",
    "print(f\"Numbner of missing values '{column}': {len(indici_nan)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no missing values for this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### birth_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1964.0\n",
      "1       1965.0\n",
      "2       1996.0\n",
      "3       1995.0\n",
      "4       1997.0\n",
      "         ...  \n",
      "6129    1966.0\n",
      "6130    1998.0\n",
      "6131    1973.0\n",
      "6132    1985.0\n",
      "6133    1999.0\n",
      "Name: birth_year, Length: 6134, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "column = 'birth_year'\n",
    "wrong_elements = df[df[column].apply(wrong_type, tipo_atteso=int)]\n",
    "\n",
    "print(wrong_elements[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the elements of birth year type are float, not int but for computation reason. Let's only check if there are elements with number after the last digit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9      NaN\n",
      "601    NaN\n",
      "894    NaN\n",
      "2408   NaN\n",
      "2515   NaN\n",
      "2536   NaN\n",
      "3046   NaN\n",
      "3551   NaN\n",
      "4142   NaN\n",
      "4384   NaN\n",
      "4756   NaN\n",
      "6072   NaN\n",
      "6080   NaN\n",
      "Name: birth_year, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "column = 'birth_year'\n",
    "decilmal_elements = df[df[column] % 1 != 0]\n",
    "\n",
    "print(decilmal_elements[column])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the birth year with a decimal part ar Nan so for this section of the analysis it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan indexes 'birth_year': [9, 601, 894, 2408, 2515, 2536, 3046, 3551, 4142, 4384, 4756, 6072, 6080]\n",
      "Numbner of missing values 'birth_year': 13\n"
     ]
    }
   ],
   "source": [
    "indici_nan = df.index[df[column].isna()].tolist()\n",
    "\n",
    "print(f\"Nan indexes '{column}': {indici_nan}\")\n",
    "print(f\"Numbner of missing values '{column}': {len(indici_nan)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Italy\n",
       "1         France\n",
       "2    Netherlands\n",
       "3        Belgium\n",
       "4          Spain\n",
       "Name: nationality, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    NaN\n",
      "Name: nationality, dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = 'nationality'\n",
    "wrong_elements = df[df[column].apply(wrong_type, tipo_atteso=str)]\n",
    "\n",
    "print(wrong_elements[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before changing the type of the column we have to remove the Nan values but it's another study case so we leave in this state. Another verification we can do is if the nationality corresponds to a real state and there are no wrong entries, to do this we can use pycountry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "nazioni_valide = {country.name.lower() for country in pycountry.countries}\n",
    "\n",
    "print(\"russian federation\" in nazioni_valide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not valid country: 310\n",
      "Not valid nationality:\n",
      "[nan, 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Yugoslavia', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Czech Republic', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Czech Republic', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Venezuela', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Russia', 'Czech Republic', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Taiwan', 'Czech Republic', 'Moldova', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Czech Republic', 'Great Britain', 'Great Britain', 'Iran', 'Great Britain', 'Great Britain', 'Great Britain', 'Venezuela', 'Russia', 'Russia', 'Venezuela', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Czech Republic', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Czech Republic', 'Great Britain', 'Iran', 'Czech Republic', 'Venezuela', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Czech Republic', 'Czech Republic', 'Czech Republic', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Venezuela', 'Moldova', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Russia', 'Russia', 'Russia', 'Venezuela', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Venezuela', 'Great Britain', 'Great Britain', 'Russia', 'Czech Republic', 'Iran', 'Russia', 'Great Britain', 'Russia', 'Czech Republic', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Czech Republic', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Czech Republic', 'Czech Republic', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Russia', 'Venezuela', 'Russia', 'Russia', 'Russia', 'Czech Republic', 'Russia', 'Russia', 'Great Britain', 'Venezuela', 'Russia', 'Czech Republic', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Czech Republic', 'Great Britain', 'Great Britain', 'Great Britain', 'Venezuela', 'Russia', 'Czech Republic', 'Czech Republic', 'Russia', 'Russia', 'Venezuela', 'Venezuela', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Russia', 'Czech Republic', 'Czech Republic', 'Great Britain', 'Czech Republic', 'Great Britain', 'Russia', 'Czech Republic', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Czech Republic', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Venezuela', 'Russia', 'Great Britain', 'Czech Republic', 'Great Britain', 'Czech Republic', 'Russia', 'Great Britain', 'Czech Republic', 'Great Britain', 'Moldova', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Czech Republic', 'Russia', 'Great Britain', 'Russia', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Venezuela', 'Russia', 'Great Britain', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Russia', 'Great Britain', 'Venezuela', 'Russia', 'Czech Republic', 'Russia', 'Russia', 'Russia', 'Great Britain', 'Great Britain', 'Great Britain', 'Great Britain', 'Russia', 'Russia', 'Czech Republic', 'Czech Republic', 'Russia', 'Russia', 'Venezuela', 'Czech Republic', 'Czech Republic', 'Russia', 'Russia', 'Hongkong', 'Russia', 'Venezuela']\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "valid_nat = {country.name.lower() for country in pycountry.countries}\n",
    "\n",
    "# Crea una lista per memorizzare le Not valid nationality\n",
    "unvalid_nat = []\n",
    "\n",
    "# Verifica le nazionalità nel DataFrame senza modificarlo\n",
    "for nat in df['nationality']:\n",
    "    if nat is not None:\n",
    "        if str(nat).lower() not in valid_nat:\n",
    "            unvalid_nat.append(nat)\n",
    "\n",
    "# Conta le Not valid nationality\n",
    "number_unvalid_nat = len(unvalid_nat)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(f\"Number of not valid country: {number_unvalid_nat}\")\n",
    "if number_unvalid_nat > 0:\n",
    "    print(\"Not valid nationality:\")\n",
    "    print(unvalid_nat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The not valid nation are in most cases real nation not considered by the library, we can add some exception to handle this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not valid nationality: 2\n",
      "Not valid nationality:\n",
      "[nan, 'Hongkong']\n"
     ]
    }
   ],
   "source": [
    "# Dizionario di corrispondenza per nazionalità comuni non standard\n",
    "corr_nat = {\n",
    "    'russia': 'russian federation',\n",
    "    'great britain': 'united kingdom',\n",
    "    'yugoslavia': 'serbia',\n",
    "    'czech republic': 'czechia',\n",
    "    'taiwan': 'taiwan, province of china',\n",
    "    'venezuela': 'venezuela, bolivarian republic of',\n",
    "    'iran': 'iran, islamic republic of',\n",
    "    'moldova': 'moldova, republic of',\n",
    "}\n",
    "\n",
    "# Crea un elenco per memorizzare le nazionalità non valide\n",
    "unvalid_nat = []\n",
    "\n",
    "for nat in df['nationality']:\n",
    "    if nat is None:\n",
    "        continue\n",
    "    nat_lower = str(nat).lower()\n",
    "    # Verifica se la nazionalità è nel dizionario di corrispondenza\n",
    "    if nat_lower in corr_nat:\n",
    "        nat_lower = corr_nat[nat_lower]\n",
    "    \n",
    "    if nat_lower not in valid_nat:\n",
    "        unvalid_nat.append(nat)\n",
    "\n",
    "# Conta le nazionalità non valide\n",
    "number_unvalid_nat = len(unvalid_nat)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(f\"Number of not valid nationality: {number_unvalid_nat}\")\n",
    "if number_unvalid_nat > 0:\n",
    "    print(\"Not valid nationality:\")\n",
    "    print(unvalid_nat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last nation which is still invalid is hong kong, in pycountry is 'hong kong' let's see if its the only one entry or if it's a typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [_url, name, birth_year, weight, height, nationality]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "hong_kong = df[df['nationality'].str.lower() == 'hong kong']\n",
    "\n",
    "print(hong_kong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a typo so we can conclude that every country is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan indexes 'nationality': [9]\n",
      "Numbner of missing values 'nationality': 1\n"
     ]
    }
   ],
   "source": [
    "indici_nan = df.index[df[column].isna()].tolist()\n",
    "\n",
    "print(f\"Nan indexes '{column}': {indici_nan}\")\n",
    "print(f\"Numbner of missing values '{column}': {len(indici_nan)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUPLICATE DATA\n",
    "For this task we can check two things:\n",
    "\n",
    "- Row which are the exact identical\n",
    "- Row with the same _url (our identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identical row: 0\n",
      "Number of row with same _url: 0\n",
      "Number of row with same name: 14\n"
     ]
    }
   ],
   "source": [
    "same_rows = df.duplicated(keep=False)  \n",
    "\n",
    "url_duplicates = df.duplicated(subset=['_url'], keep=False) \n",
    "\n",
    "name_duplicates = df.duplicated(subset=['name'], keep=False) \n",
    "\n",
    "\n",
    "print(f\"Number of identical row: {same_rows.sum()}\")\n",
    "print(f\"Number of row with same _url: {url_duplicates.sum()}\")\n",
    "print(f\"Number of row with same name: {name_duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of identical row is zero but we found 14 rows with same name, let's check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name                        _url\n",
      "347        Andrea  Peron              andrea-peron-1\n",
      "1745    Roman  Kreuziger          roman-kreuziger-sr\n",
      "2235   Alessandro  Pozzi           alessandro-pozzi2\n",
      "2601    Roman  Kreuziger             roman-kreuziger\n",
      "2682       Andrea  Peron                andrea-peron\n",
      "2862    Antonio  Cabello       antonio-cabello-baena\n",
      "2939        Jesús  López               jesus-lopez23\n",
      "2953  Alberto  Fernández     alberto-fernandez-sainz\n",
      "3238    Antonio  Cabello             antonio-cabello\n",
      "4917   Sergio  Domínguez  sergio-dominguez-rodriguez\n",
      "4919   Sergio  Domínguez      sergio-dominguez-munoz\n",
      "5040        Jesús  López          jesus-lopez-carril\n",
      "5720  Alberto  Fernández    alberto-fernandez-blanco\n",
      "5722   Alessandro  Pozzi            alessandro-pozzi\n"
     ]
    }
   ],
   "source": [
    "print(df[name_duplicates][['name', '_url']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the disambuigation is not so trivial, some times the duplicates are converted with a number after the classical url convertion, other ones it's converted with different information (second name). This makes the direct conversion harder but if we consider only the url as the identifier we have a well defined dataset in this case. Suggestion: redifinition of the url identifier in a clearly way :\n",
    "\n",
    "Name Second Surname in name-second-surname(number if there are identical name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
