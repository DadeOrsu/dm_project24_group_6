{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task no. 4\n",
    "\n",
    "This is the notebook taking care of the task no.4 : feature prediction.\n",
    "\n",
    "In our analysis we have decided very different models to see how they differ from eachother in term of performance and also their explainability to see if they make sense.\n",
    "\n",
    "For this notebook we decided to explore the nature of NeuralNetworks on the dataset.\n",
    "\n",
    "First a preparation is due to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "races_final_path = os.path.join('..','dataset', 'engineered_races.csv')\n",
    "cyclists_final_path = os.path.join('..','dataset', 'cyclists_final_enhanced.csv')\n",
    "\n",
    "\n",
    "cyclists_data = pd.read_csv(cyclists_final_path)\n",
    "races_data = pd.read_csv(races_final_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we binarize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cyclists_data.rename(columns={'name': 'cyclist'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_data = races_data.merge(cyclists_data, left_on='cyclist', right_on='_url', how='inner')\n",
    "\n",
    "merged_data['top_20'] = merged_data['position'].apply(lambda x: 1 if x <= 20 else 0)\n",
    "\n",
    "\n",
    "merged_data['date'] = pd.to_datetime(merged_data['date'])\n",
    "\n",
    "columns_to_keep = [\n",
    "\n",
    "    'bmi','career_points','career_duration(days)','debut_year', # cyclists features\n",
    "    'points','difficulty_score','competitive_age','climbing_efficiency', # races features\n",
    "    'top_20'# target feature\n",
    "]\n",
    "\n",
    "train_set = merged_data[merged_data['date'] < '2022-01-01']\n",
    "test_set = merged_data[merged_data['date'] >= '2022-01-01']\n",
    "std_scaler= StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "test_set = test_set[columns_to_keep]\n",
    "train_set = train_set[columns_to_keep]\n",
    "\n",
    "X_dev = train_set.drop(columns=['top_20'])\n",
    "\n",
    "X_test = test_set.drop(columns=['top_20'])\n",
    "y_test = test_set['top_20']\n",
    "\n",
    "X_test= std_scaler.fit_transform(X_test)\n",
    "X_dev= std_scaler.fit_transform(X_dev)\n",
    "\n",
    "y_dev = train_set['top_20']\n",
    "X_train,X_val,Y_train,Y_val=train_test_split(\n",
    "    X_dev,y_dev,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_dev\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first a stratification can only help the generization capabilities.\n",
    "\n",
    "Now we have to setup the task, for this kind of setting the binary cross entropy is the most appropriate given we just want to classify stuff and we are not doing any regression whatsoever.\n",
    "\n",
    "A first test using a simple NN might be usefull in this case to see the most basic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:55:26.829562: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 23:55:26.838358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 23:55:26.845097: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 23:55:26.846984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 23:55:26.852384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733871328.061873   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.086190   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.086435   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.088360   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.088523   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.088656   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.201310   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733871328.201453   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-10 23:55:28.201464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1733871328.201572   11047 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-10 23:55:28.201595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5529 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:03:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers, models, initializers\n",
    "from keras.optimizers import Adam, SGD\n",
    "import itertools as it\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import GlorotUniform, GlorotNormal,HeNormal,HeUniform\n",
    "initializer=initializers.HeNormal()\n",
    "\n",
    "\n",
    "def get_device_auto():\n",
    "    gpus_list=tf.config.list_physical_devices('GPU')\n",
    "    device = None\n",
    "    if len(gpus_list) != 0:\n",
    "        device=gpus_list[0]\n",
    "    else:\n",
    "        device=tf.config.list_physical_devices('CPU')[0]\n",
    "    return device\n",
    "\n",
    "def create_ff_nn(\n",
    "        optimizer=Adam(),\n",
    "        num_layers=2,\n",
    "        num_units=64,\n",
    "        input_dim=256,\n",
    "        hidden_activation='relu',\n",
    "        output_activation='sigmoid',\n",
    "        loss_function='binary_crossentropy',\n",
    "        metrics=['accuracy','f1_score','binary_crossentropy'],\n",
    "        learning_rate=0.001\n",
    "        ):\n",
    "    model=models.Sequential()\n",
    "    optimizer = Adam()\n",
    "    model.add(layers.Dense(num_units,input_dim=input_dim,activation=hidden_activation))\n",
    "    for _ in range(num_layers -1):\n",
    "        model.add(layers.Dense(\n",
    "            num_units,\n",
    "            activation=hidden_activation,\n",
    "            kernel_initializer=HeNormal()\n",
    "            ))\n",
    "    model.add(layers.Dense(\n",
    "        1,\n",
    "        activation=output_activation,\n",
    "        kernel_initializer=GlorotNormal()\n",
    "        ))\n",
    "    optimizer.learning_rate=learning_rate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def hyperparams_iterator(hyperparams):\n",
    "    return map(\n",
    "        lambda comb:  {k:v for k,v in zip(hyperparams.keys(),comb)},\n",
    "        it.product(*hyperparams.values())\n",
    "    )\n",
    "\n",
    "early_stopping=EarlyStopping(\n",
    "    monitor='f1_score',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "hyperparams={\n",
    "    'num_layers':[5,10,15,20],\n",
    "    'learning_rate':[0.001,0.0001],\n",
    "    'num_units':[1024,2048,4096]\n",
    "}\n",
    "\n",
    "device=get_device_auto()\n",
    "batch_size=1024\n",
    "tf.random.set_seed(42)\n",
    "best_val=float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1733871329.613099   11143 service.cc:146] XLA service 0x7f3b7401a670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733871329.613135   11143 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2024-12-10 23:55:29.632070: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-10 23:55:29.741586: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90101\n",
      "2024-12-10 23:55:30.412448: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_54', 304 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:30.874363: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:30.908352: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 344 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.038648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.315240: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_582', 208 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.403814: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 376 bytes spill stores, 424 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.412189: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.455868: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.461625: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_54', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:31.553927: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_582', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 51/434\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7866 - binary_crossentropy: 0.4739 - f1_score: 0.2841 - loss: 0.4725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733871332.264390   11143 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m421/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8319 - binary_crossentropy: 0.4218 - f1_score: 0.2889 - loss: 0.4216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:55:34.208041: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:34.568671: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:34.620525: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 712 bytes spill stores, 880 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:34.665724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:34.684536: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_253', 600 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:34.911233: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_582', 300 bytes spill stores, 308 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8323 - binary_crossentropy: 0.4213 - f1_score: 0.2889 - loss: 0.4213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:55:36.628902: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:36.854658: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 712 bytes spill stores, 880 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:36.895984: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:36.926832: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2024-12-10 23:55:37.026388: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 600 bytes spill stores, 404 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8323 - binary_crossentropy: 0.4212 - f1_score: 0.2889 - loss: 0.4212 - val_accuracy: 0.8447 - val_binary_crossentropy: 0.3988 - val_f1_score: 0.2894 - val_loss: 0.3988\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8455 - binary_crossentropy: 0.3952 - f1_score: 0.2883 - loss: 0.3952\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8445 - binary_crossentropy: 0.3986 - f1_score: 0.2898 - loss: 0.3986\n",
      "{'num_layers': 5, 'learning_rate': 0.001, 'num_units': 1024, 'f1_score_train': 0.289430171251297, 'accuracy_train': 0.8448644876480103, 'bin_cross_ent_train': 0.39629536867141724, 'f1_score_val': 0.28942960500717163, 'accuracy_val': 0.8447408080101013, 'bin_cross_ent_val': 0.3987657427787781}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <KerasVariable shape=(8, 2048), dtype=float32, path=sequential_1/dense_6/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m hyperparams_iterator(hyperparams):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mcreate_ff_nn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     new_row\u001b[38;5;241m=\u001b[39mparams\n\u001b[1;32m     14\u001b[0m     eval_results\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mevaluate(X_train,Y_train,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:290\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[0;32m--> 290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown variable: <KerasVariable shape=(8, 2048), dtype=float32, path=sequential_1/dense_6/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "\n",
    "with tf.device(device.device_type):\n",
    "    for params in hyperparams_iterator(hyperparams):\n",
    "        model=create_ff_nn(**params,input_dim=X_train.shape[1])\n",
    "        model.fit(\n",
    "            X_train,Y_train,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val,Y_val),\n",
    "            callbacks=[early_stopping]\n",
    "            )\n",
    "        new_row=params\n",
    "        \n",
    "        eval_results=model.evaluate(X_train,Y_train,batch_size=batch_size,return_dict=True)\n",
    "        f1_score,accuracy,bin_cross_ent=eval_results['f1_score'],eval_results['accuracy'],eval_results['binary_crossentropy']\n",
    "        new_row|={\n",
    "            'f1_score_train':f1_score,\n",
    "            'accuracy_train':accuracy,\n",
    "            'bin_cross_ent_train':bin_cross_ent,\n",
    "            }\n",
    "        eval_results=model.evaluate(X_val,Y_val,batch_size=batch_size,return_dict=True)\n",
    "        f1_score,accuracy,bin_cross_ent=eval_results['f1_score'],eval_results['accuracy'],eval_results['binary_crossentropy']\n",
    "        new_row|={\n",
    "            'f1_score_val':f1_score,\n",
    "            'accuracy_val':accuracy,\n",
    "            'bin_cross_ent_val':bin_cross_ent,\n",
    "            }\n",
    "        if bin_cross_ent < best_val:\n",
    "            best_val = bin_cross_ent\n",
    "            model.save('weights/best_ff_nn.h5')\n",
    "        print(new_row)\n",
    "        results.append(new_row)\n",
    "pd_results=pd.DataFrame(results)\n",
    "\n",
    "pd_results.sort_values(by='bin_cross_ent_val')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
