{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "This notebook uses DBSCAN as a clustering density based approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>std_name</th>\n",
       "      <th>cyclist</th>\n",
       "      <th>profile</th>\n",
       "      <th>is_tarmac</th>\n",
       "      <th>difficulty_level</th>\n",
       "      <th>points</th>\n",
       "      <th>length</th>\n",
       "      <th>climb_total</th>\n",
       "      <th>competitive_age</th>\n",
       "      <th>...</th>\n",
       "      <th>cyclist_age</th>\n",
       "      <th>position</th>\n",
       "      <th>cyclist_team</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>decade</th>\n",
       "      <th>easy</th>\n",
       "      <th>hard</th>\n",
       "      <th>moderate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>result</td>\n",
       "      <td>omloop-het-nieuwsblad</td>\n",
       "      <td>andre-dierickx</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2</td>\n",
       "      <td>spain-1991</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>result</td>\n",
       "      <td>omloop-het-nieuwsblad</td>\n",
       "      <td>christian-callens</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9</td>\n",
       "      <td>free-agent</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>result</td>\n",
       "      <td>omloop-het-nieuwsblad</td>\n",
       "      <td>daniel-van-ryckeghem</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "      <td>norway-1987</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>result</td>\n",
       "      <td>omloop-het-nieuwsblad</td>\n",
       "      <td>eddy-merckx</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>team-monex-2005</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>result</td>\n",
       "      <td>omloop-het-nieuwsblad</td>\n",
       "      <td>englebert-opdebeeck</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36</td>\n",
       "      <td>free-agent</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589734</th>\n",
       "      <td>result</td>\n",
       "      <td>san-sebastian</td>\n",
       "      <td>txomin-juaristi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230300.0</td>\n",
       "      <td>4057.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>91</td>\n",
       "      <td>norway-2021</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589735</th>\n",
       "      <td>result</td>\n",
       "      <td>san-sebastian</td>\n",
       "      <td>urko-berrade-fernandez</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230300.0</td>\n",
       "      <td>4057.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33</td>\n",
       "      <td>atala-1985</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589736</th>\n",
       "      <td>result</td>\n",
       "      <td>san-sebastian</td>\n",
       "      <td>victor-de-la-parte</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230300.0</td>\n",
       "      <td>4057.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>88</td>\n",
       "      <td>c-a-1978</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589737</th>\n",
       "      <td>result</td>\n",
       "      <td>san-sebastian</td>\n",
       "      <td>welay-hagos-berhe</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230300.0</td>\n",
       "      <td>4057.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>40</td>\n",
       "      <td>bankgiroloterij-batavus-2000</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589738</th>\n",
       "      <td>result</td>\n",
       "      <td>san-sebastian</td>\n",
       "      <td>xabier-berasategi-garmendia</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230300.0</td>\n",
       "      <td>4057.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>87</td>\n",
       "      <td>norway-2021</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>589739 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stage               std_name                      cyclist  profile  \\\n",
       "0       result  omloop-het-nieuwsblad               andre-dierickx      3.0   \n",
       "1       result  omloop-het-nieuwsblad            christian-callens      3.0   \n",
       "2       result  omloop-het-nieuwsblad         daniel-van-ryckeghem      3.0   \n",
       "3       result  omloop-het-nieuwsblad                  eddy-merckx      3.0   \n",
       "4       result  omloop-het-nieuwsblad          englebert-opdebeeck      3.0   \n",
       "...        ...                    ...                          ...      ...   \n",
       "589734  result          san-sebastian              txomin-juaristi      2.0   \n",
       "589735  result          san-sebastian       urko-berrade-fernandez      2.0   \n",
       "589736  result          san-sebastian           victor-de-la-parte      2.0   \n",
       "589737  result          san-sebastian            welay-hagos-berhe      2.0   \n",
       "589738  result          san-sebastian  xabier-berasategi-garmendia      2.0   \n",
       "\n",
       "        is_tarmac difficulty_level  points    length  climb_total  \\\n",
       "0           False         moderate   125.0  195000.0  2330.469215   \n",
       "1           False         moderate   125.0  195000.0  2330.469215   \n",
       "2           False         moderate   125.0  195000.0  2330.469215   \n",
       "3           False         moderate   125.0  195000.0  2330.469215   \n",
       "4           False         moderate   125.0  195000.0  2330.469215   \n",
       "...           ...              ...     ...       ...          ...   \n",
       "589734      False         moderate   225.0  230300.0  4057.000000   \n",
       "589735      False         moderate   225.0  230300.0  4057.000000   \n",
       "589736      False         moderate   225.0  230300.0  4057.000000   \n",
       "589737      False         moderate   225.0  230300.0  4057.000000   \n",
       "589738      False         moderate   225.0  230300.0  4057.000000   \n",
       "\n",
       "        competitive_age  ...  cyclist_age  position  \\\n",
       "0                  23.0  ...         23.0         2   \n",
       "1                  23.0  ...         23.0         9   \n",
       "2                  25.0  ...         25.0         5   \n",
       "3                  25.0  ...         25.0         6   \n",
       "4                  24.0  ...         24.0        36   \n",
       "...                 ...  ...          ...       ...   \n",
       "589734             28.0  ...         28.0        91   \n",
       "589735             26.0  ...         26.0        33   \n",
       "589736             37.0  ...         37.0        88   \n",
       "589737             22.0  ...         22.0        40   \n",
       "589738             23.0  ...         23.0        87   \n",
       "\n",
       "                        cyclist_team  day  month  year  decade  easy  hard  \\\n",
       "0                         spain-1991   28      2  1970     NaN   1.0   0.0   \n",
       "1                         free-agent   28      2  1970     NaN   1.0   0.0   \n",
       "2                        norway-1987   28      2  1970     NaN   1.0   0.0   \n",
       "3                    team-monex-2005   28      2  1970     NaN   1.0   0.0   \n",
       "4                         free-agent   28      2  1970     NaN   1.0   0.0   \n",
       "...                              ...  ...    ...   ...     ...   ...   ...   \n",
       "589734                   norway-2021   29      7  2023     NaN   1.0   0.0   \n",
       "589735                    atala-1985   29      7  2023     NaN   1.0   0.0   \n",
       "589736                      c-a-1978   29      7  2023     NaN   1.0   0.0   \n",
       "589737  bankgiroloterij-batavus-2000   29      7  2023     NaN   1.0   0.0   \n",
       "589738                   norway-2021   29      7  2023     NaN   1.0   0.0   \n",
       "\n",
       "       moderate  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "589734      0.0  \n",
       "589735      0.0  \n",
       "589736      0.0  \n",
       "589737      0.0  \n",
       "589738      0.0  \n",
       "\n",
       "[589739 rows x 27 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def most_frequent(series):\n",
    "    return series.mode()[0] if not series.mode().empty else series.iloc[0]\n",
    "\n",
    "\n",
    "RACES_PATH=path.join(\"..\",\"dataset\",\"engineered_races.csv\")\n",
    "\n",
    "races_df=pd.read_csv(RACES_PATH)\n",
    "\n",
    "#print(races_df.describe())\n",
    "\n",
    "\n",
    "\n",
    "#clustering_data=clustering_data.drop(columns=[\"date\",\"difficulty_level\"])\n",
    "#print(races_df.info())\n",
    "clustering_data=races_df.groupby(['date','stage','std_name','cyclist']).agg({\n",
    "    'profile':most_frequent,\n",
    "    'is_tarmac':most_frequent,\n",
    "    'difficulty_level':most_frequent,\n",
    "\n",
    "    'points':'sum',\n",
    "\n",
    "    'length':'mean',\n",
    "    'climb_total':'mean',\n",
    "    'competitive_age':'mean',\n",
    "    'startlist_quality':'mean',\n",
    "    'delta':'mean',\n",
    "    'performance_index':'mean',\n",
    "    'difficulty':'mean',\n",
    "    'convenience_score':'mean',\n",
    "    'difficulty_score':'mean',\n",
    "    'gain_ratio':'mean',\n",
    "\n",
    "    'cyclist_age':'first',\n",
    "    'position':'first',\n",
    "    'cyclist_team':'first',\n",
    "}).reset_index()\n",
    "\n",
    "#clustering_data=races_df[cols].copy()\n",
    "#convert to timestamp(units are useless since it's getting normalized)\n",
    "clustering_data['date']=pd.to_datetime(clustering_data['date'])\n",
    "clustering_data['day']=clustering_data['date'].dt.day\n",
    "clustering_data['month']=clustering_data['date'].dt.month\n",
    "clustering_data['year']=clustering_data['date'].dt.year\n",
    "\n",
    "#one hot encoding difficulty\n",
    "ohe_diff_lvl=pd.get_dummies(races_df['difficulty_level']).astype(float)\n",
    "\n",
    "#clustering_data\n",
    "dec_cut=pd.date_range(\n",
    "    start=clustering_data['date'].min(),\n",
    "    end=clustering_data['date'].max(),\n",
    "    freq='2YE'\n",
    ")\n",
    "clustering_data['decade']=pd.cut(\n",
    "    clustering_data['date'],\n",
    "    bins=dec_cut,\n",
    ")\n",
    "\n",
    "clustering_data[ohe_diff_lvl.columns]=ohe_diff_lvl\n",
    "\n",
    "clustering_data=clustering_data.drop(columns=\"date\")\n",
    "\n",
    "clustering_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before starting, for reasons of time we could,for this delivery, do the clustering on the full dataset so for now we decided to employ some sort of data reduction as to make it feasible to run such an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering organization\n",
    "\n",
    "For reasons of time using DBSCAN on the whole dataset is not feasible, a second approach would be to try and a segmentation, for this part we wanted to employ a clusterization that is time based and analyses clusters across decades and see what we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_sampling_reduce(data,reduction_percent):\n",
    "    num_samples=data.shape[0]\n",
    "\n",
    "    reduction_num_samples=int(np.ceil(reduction_percent*num_samples))\n",
    "\n",
    "    RANDOM_SEED=42\n",
    "\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    reduction_idx=np.random.choice(range(len(clustering_data)),reduction_num_samples,replace=False)\n",
    "\n",
    "    return data.iloc[reduction_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes are due before starting, first the eps are difficulty to setup for now a good strategy would be to take inspiration using the first paper the introduced the algorithm, which you can find [here](https://dl.acm.org/doi/10.5555/3001460.3001507), and use the distance from the k-th NN varying K until we find a good eps value for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have highly dimensional features just trying the values isn't enough. A good idea would be to use a KNN and the elbow method to estimate the correct eps value. We still need to do some kind of aggregation before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## applying the elbow method\n",
    "so given the unfeasibility of using the whole dataset for this part we divide everything going from decade to decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['profile', 'points', 'length', 'climb_total', 'competitive_age',\n",
      "       'startlist_quality', 'delta', 'performance_index', 'difficulty',\n",
      "       'convenience_score', 'cyclist_age', 'day', 'month', 'year', 'decade'],\n",
      "      dtype='object')\n",
      "{Interval(1970-12-31 00:00:00, 1972-12-31 00:00:00, closed='right'): 753, Interval(1972-12-31 00:00:00, 1974-12-31 00:00:00, closed='right'): 459, Interval(1974-12-31 00:00:00, 1976-12-31 00:00:00, closed='right'): 1233, Interval(1976-12-31 00:00:00, 1978-12-31 00:00:00, closed='right'): 3686, Interval(1978-12-31 00:00:00, 1980-12-31 00:00:00, closed='right'): 7789, Interval(1980-12-31 00:00:00, 1982-12-31 00:00:00, closed='right'): 9836, Interval(1982-12-31 00:00:00, 1984-12-31 00:00:00, closed='right'): 11363, Interval(1984-12-31 00:00:00, 1986-12-31 00:00:00, closed='right'): 11256, Interval(1986-12-31 00:00:00, 1988-12-31 00:00:00, closed='right'): 10791, Interval(1988-12-31 00:00:00, 1990-12-31 00:00:00, closed='right'): 10995, Interval(1990-12-31 00:00:00, 1992-12-31 00:00:00, closed='right'): 11885, Interval(1992-12-31 00:00:00, 1994-12-31 00:00:00, closed='right'): 14021, Interval(1994-12-31 00:00:00, 1996-12-31 00:00:00, closed='right'): 17497, Interval(1996-12-31 00:00:00, 1998-12-31 00:00:00, closed='right'): 23714, Interval(1998-12-31 00:00:00, 2000-12-31 00:00:00, closed='right'): 31654, Interval(2000-12-31 00:00:00, 2002-12-31 00:00:00, closed='right'): 35366, Interval(2002-12-31 00:00:00, 2004-12-31 00:00:00, closed='right'): 32886, Interval(2004-12-31 00:00:00, 2006-12-31 00:00:00, closed='right'): 35188, Interval(2006-12-31 00:00:00, 2008-12-31 00:00:00, closed='right'): 37490, Interval(2008-12-31 00:00:00, 2010-12-31 00:00:00, closed='right'): 36861, Interval(2010-12-31 00:00:00, 2012-12-31 00:00:00, closed='right'): 39798, Interval(2012-12-31 00:00:00, 2014-12-31 00:00:00, closed='right'): 40272, Interval(2014-12-31 00:00:00, 2016-12-31 00:00:00, closed='right'): 40107, Interval(2016-12-31 00:00:00, 2018-12-31 00:00:00, closed='right'): 38919, Interval(2018-12-31 00:00:00, 2020-12-31 00:00:00, closed='right'): 29676, Interval(2020-12-31 00:00:00, 2022-12-31 00:00:00, closed='right'): 39699}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28908/1218421442.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  dec_groups=clustering_data.groupby('decade')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of groups=26\n",
      "initial eps values per group={Interval(1970-12-31 00:00:00, 1972-12-31 00:00:00, closed='right'): 0.9155102427433038, Interval(1972-12-31 00:00:00, 1974-12-31 00:00:00, closed='right'): 1.3017056548170938, Interval(1974-12-31 00:00:00, 1976-12-31 00:00:00, closed='right'): 0.8180223071229451, Interval(1976-12-31 00:00:00, 1978-12-31 00:00:00, closed='right'): 0.5231390365005472, Interval(1978-12-31 00:00:00, 1980-12-31 00:00:00, closed='right'): 0.4627659081819203, Interval(1980-12-31 00:00:00, 1982-12-31 00:00:00, closed='right'): 0.4631630266953081, Interval(1982-12-31 00:00:00, 1984-12-31 00:00:00, closed='right'): 0.4465417852131229, Interval(1984-12-31 00:00:00, 1986-12-31 00:00:00, closed='right'): 0.45026786677913466, Interval(1986-12-31 00:00:00, 1988-12-31 00:00:00, closed='right'): 0.46040209204023885, Interval(1988-12-31 00:00:00, 1990-12-31 00:00:00, closed='right'): 0.4565313815062814, Interval(1990-12-31 00:00:00, 1992-12-31 00:00:00, closed='right'): 0.4786168165221251, Interval(1992-12-31 00:00:00, 1994-12-31 00:00:00, closed='right'): 0.42762980772458675, Interval(1994-12-31 00:00:00, 1996-12-31 00:00:00, closed='right'): 0.44861991311204447, Interval(1996-12-31 00:00:00, 1998-12-31 00:00:00, closed='right'): 0.3976652322379788, Interval(1998-12-31 00:00:00, 2000-12-31 00:00:00, closed='right'): 0.37367426863547687, Interval(2000-12-31 00:00:00, 2002-12-31 00:00:00, closed='right'): 0.4308319127968153, Interval(2002-12-31 00:00:00, 2004-12-31 00:00:00, closed='right'): 0.41836523567552414, Interval(2004-12-31 00:00:00, 2006-12-31 00:00:00, closed='right'): 0.3804681918708554, Interval(2006-12-31 00:00:00, 2008-12-31 00:00:00, closed='right'): 0.36436437716985803, Interval(2008-12-31 00:00:00, 2010-12-31 00:00:00, closed='right'): 0.3828759503297802, Interval(2010-12-31 00:00:00, 2012-12-31 00:00:00, closed='right'): 0.38822123840425277, Interval(2012-12-31 00:00:00, 2014-12-31 00:00:00, closed='right'): 0.37466690615416265, Interval(2014-12-31 00:00:00, 2016-12-31 00:00:00, closed='right'): 0.36700687779115126, Interval(2016-12-31 00:00:00, 2018-12-31 00:00:00, closed='right'): 0.38243514750585933, Interval(2018-12-31 00:00:00, 2020-12-31 00:00:00, closed='right'): 0.4056780838070035, Interval(2020-12-31 00:00:00, 2022-12-31 00:00:00, closed='right'): 0.40035514020418617}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "clustering_data=clustering_data.drop(columns=[\"difficulty_level\",\"stage\",\"std_name\",\"cyclist\",\"cyclist_team\",\"easy\",\"hard\",\"moderate\",\"is_tarmac\",\"gain_ratio\",\"difficulty_score\",\"position\"]).drop_duplicates()\n",
    "\n",
    "#clustering_data=random_sampling_reduce(clustering_data,1)\n",
    "\n",
    "std_scaler=StandardScaler()\n",
    "\n",
    "print(clustering_data.columns)\n",
    "\n",
    "dec_groups=clustering_data.groupby('decade')\n",
    "normalized_decade_groups={k:std_scaler.fit_transform(g.drop(columns=\"decade\").drop_duplicates()) for k,g in dec_groups }\n",
    "\n",
    "print({k:len(g) for k,g in normalized_decade_groups.items()})\n",
    "\n",
    "initial_eps=dict()\n",
    "\n",
    "kth_neighbor=30\n",
    "\n",
    "for k,data in normalized_decade_groups.items():\n",
    "    min_pts=data.shape[1]\n",
    "    nn=NearestNeighbors(n_neighbors=min_pts-1,n_jobs=-1)\n",
    "    nn.fit(data)\n",
    "    distances,indices= nn.kneighbors(data)\n",
    "    k_distances= np.sort(distances[:, -1])\n",
    "\n",
    "    initial_eps[k]=k_distances[kth_neighbor-1]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "number of groups={len(normalized_decade_groups)}\n",
    "initial eps values per group={initial_eps}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the sorted distances we can pick the eps values and proceed to test dbscan , in this case we have more starting eps values given the segmentation hance we have a lot of tests to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    period (1970-12-31 00:00:00, 1972-12-31 00:00:00]\n",
      "    maxium distance: 26.4432145906155\n",
      "    average concentration:28.476114256820882\n",
      "    eps values:[9.15510243e+00 4.57755121e+00 2.28877561e+00 9.15510243e-01\n",
      " 9.15510243e-02 9.15510243e-03 9.15510243e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:752\n",
      "    number of samples used:753\n",
      "    \n",
      "-0 - (9.155102427433038, 'euclidean', 752)\n",
      "dbscan done, time=0.02748727798461914 seconds | silhoutte score:0.7908954023847292\n",
      "-1 - (4.577551213716519, 'euclidean', 752)\n",
      "dbscan done, time=0.020033597946166992 seconds | silhoutte score:all noise\n",
      "-2 - (2.2887756068582594, 'euclidean', 752)\n",
      "dbscan done, time=0.019155502319335938 seconds | silhoutte score:all noise\n",
      "-3 - (0.9155102427433038, 'euclidean', 752)\n",
      "dbscan done, time=0.018077850341796875 seconds | silhoutte score:all noise\n",
      "-4 - (0.09155102427433039, 'euclidean', 752)\n",
      "dbscan done, time=0.017905712127685547 seconds | silhoutte score:all noise\n",
      "-5 - (0.009155102427433039, 'euclidean', 752)\n",
      "dbscan done, time=0.015729427337646484 seconds | silhoutte score:all noise\n",
      "-6 - (9.15510242743304e-05, 'euclidean', 752)\n",
      "dbscan done, time=0.01566290855407715 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1972-12-31 00:00:00, 1974-12-31 00:00:00]\n",
      "    maxium distance: 12.273333126551666\n",
      "    average concentration:37.39815380770662\n",
      "    eps values:[1.30170565e+01 6.50852827e+00 3.25426414e+00 1.30170565e+00\n",
      " 1.30170565e-01 1.30170565e-02 1.30170565e-04]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:458\n",
      "    number of samples used:459\n",
      "    \n",
      "-0 - (13.017056548170938, 'euclidean', 458)\n",
      "dbscan done, time=0.006284475326538086 seconds | silhoutte score:all noise\n",
      "-1 - (6.508528274085469, 'euclidean', 458)\n",
      "dbscan done, time=0.006350040435791016 seconds | silhoutte score:all noise\n",
      "-2 - (3.2542641370427345, 'euclidean', 458)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n",
      "/home/mirdan08/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:246: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan done, time=0.0058727264404296875 seconds | silhoutte score:all noise\n",
      "-3 - (1.3017056548170938, 'euclidean', 458)\n",
      "dbscan done, time=0.006792306900024414 seconds | silhoutte score:all noise\n",
      "-4 - (0.13017056548170938, 'euclidean', 458)\n",
      "dbscan done, time=0.006266355514526367 seconds | silhoutte score:all noise\n",
      "-5 - (0.013017056548170938, 'euclidean', 458)\n",
      "dbscan done, time=0.0057904720306396484 seconds | silhoutte score:all noise\n",
      "-6 - (0.00013017056548170937, 'euclidean', 458)\n",
      "dbscan done, time=0.005656719207763672 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1974-12-31 00:00:00, 1976-12-31 00:00:00]\n",
      "    maxium distance: 11.144002200913832\n",
      "    average concentration:110.64247635368301\n",
      "    eps values:[8.18022307e+00 4.09011154e+00 2.04505577e+00 8.18022307e-01\n",
      " 8.18022307e-02 8.18022307e-03 8.18022307e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:1232\n",
      "    number of samples used:1233\n",
      "    \n",
      "-0 - (8.18022307122945, 'euclidean', 1232)\n",
      "dbscan done, time=0.03167438507080078 seconds | silhoutte score:all noise\n",
      "-1 - (4.090111535614725, 'euclidean', 1232)\n",
      "dbscan done, time=0.02861809730529785 seconds | silhoutte score:all noise\n",
      "-2 - (2.0450557678073626, 'euclidean', 1232)\n",
      "dbscan done, time=0.021091938018798828 seconds | silhoutte score:all noise\n",
      "-3 - (0.8180223071229451, 'euclidean', 1232)\n",
      "dbscan done, time=0.023488998413085938 seconds | silhoutte score:all noise\n",
      "-4 - (0.08180223071229452, 'euclidean', 1232)\n",
      "dbscan done, time=0.021839141845703125 seconds | silhoutte score:all noise\n",
      "-5 - (0.008180223071229452, 'euclidean', 1232)\n",
      "dbscan done, time=0.02125263214111328 seconds | silhoutte score:all noise\n",
      "-6 - (8.180223071229451e-05, 'euclidean', 1232)\n",
      "dbscan done, time=0.021573781967163086 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1976-12-31 00:00:00, 1978-12-31 00:00:00]\n",
      "    maxium distance: 15.21115108721325\n",
      "    average concentration:242.32222656038923\n",
      "    eps values:[5.23139037e+00 2.61569518e+00 1.30784759e+00 5.23139037e-01\n",
      " 5.23139037e-02 5.23139037e-03 5.23139037e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:3685\n",
      "    number of samples used:3686\n",
      "    \n",
      "-0 - (5.231390365005471, 'euclidean', 3685)\n",
      "dbscan done, time=0.25864720344543457 seconds | silhoutte score:all noise\n",
      "-1 - (2.6156951825027357, 'euclidean', 3685)\n",
      "dbscan done, time=0.21746182441711426 seconds | silhoutte score:all noise\n",
      "-2 - (1.3078475912513678, 'euclidean', 3685)\n",
      "dbscan done, time=0.21209144592285156 seconds | silhoutte score:all noise\n",
      "-3 - (0.5231390365005472, 'euclidean', 3685)\n",
      "dbscan done, time=0.21070075035095215 seconds | silhoutte score:all noise\n",
      "-4 - (0.05231390365005472, 'euclidean', 3685)\n",
      "dbscan done, time=0.2105088233947754 seconds | silhoutte score:all noise\n",
      "-5 - (0.005231390365005472, 'euclidean', 3685)\n",
      "dbscan done, time=0.21001601219177246 seconds | silhoutte score:all noise\n",
      "-6 - (5.231390365005472e-05, 'euclidean', 3685)\n",
      "dbscan done, time=0.23573899269104004 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1978-12-31 00:00:00, 1980-12-31 00:00:00]\n",
      "    maxium distance: 46.239178839294205\n",
      "    average concentration:168.45022328512638\n",
      "    eps values:[4.62765908e+00 2.31382954e+00 1.15691477e+00 4.62765908e-01\n",
      " 4.62765908e-02 4.62765908e-03 4.62765908e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:7788\n",
      "    number of samples used:7789\n",
      "    \n",
      "-0 - (4.627659081819203, 'euclidean', 7788)\n",
      "dbscan done, time=1.1977992057800293 seconds | silhoutte score:all noise\n",
      "-1 - (2.3138295409096017, 'euclidean', 7788)\n",
      "dbscan done, time=0.9943792819976807 seconds | silhoutte score:all noise\n",
      "-2 - (1.1569147704548008, 'euclidean', 7788)\n",
      "dbscan done, time=1.1125130653381348 seconds | silhoutte score:all noise\n",
      "-3 - (0.4627659081819203, 'euclidean', 7788)\n",
      "dbscan done, time=1.0093915462493896 seconds | silhoutte score:all noise\n",
      "-4 - (0.046276590818192034, 'euclidean', 7788)\n",
      "dbscan done, time=1.0988538265228271 seconds | silhoutte score:all noise\n",
      "-5 - (0.0046276590818192034, 'euclidean', 7788)\n",
      "dbscan done, time=0.9861218929290771 seconds | silhoutte score:all noise\n",
      "-6 - (4.6276590818192034e-05, 'euclidean', 7788)\n",
      "dbscan done, time=1.0618722438812256 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1980-12-31 00:00:00, 1982-12-31 00:00:00]\n",
      "    maxium distance: 25.54733352820959\n",
      "    average concentration:385.0108266343727\n",
      "    eps values:[4.63163027e+00 2.31581513e+00 1.15790757e+00 4.63163027e-01\n",
      " 4.63163027e-02 4.63163027e-03 4.63163027e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:9835\n",
      "    number of samples used:9836\n",
      "    \n",
      "-0 - (4.631630266953081, 'euclidean', 9835)\n",
      "dbscan done, time=1.8882136344909668 seconds | silhoutte score:all noise\n",
      "-1 - (2.3158151334765407, 'euclidean', 9835)\n",
      "dbscan done, time=1.5842046737670898 seconds | silhoutte score:all noise\n",
      "-2 - (1.1579075667382703, 'euclidean', 9835)\n",
      "dbscan done, time=1.6225740909576416 seconds | silhoutte score:all noise\n",
      "-3 - (0.4631630266953081, 'euclidean', 9835)\n",
      "dbscan done, time=1.5684823989868164 seconds | silhoutte score:all noise\n",
      "-4 - (0.046316302669530816, 'euclidean', 9835)\n",
      "dbscan done, time=1.5396578311920166 seconds | silhoutte score:all noise\n",
      "-5 - (0.0046316302669530815, 'euclidean', 9835)\n",
      "dbscan done, time=1.4261858463287354 seconds | silhoutte score:all noise\n",
      "-6 - (4.631630266953081e-05, 'euclidean', 9835)\n",
      "dbscan done, time=1.5406742095947266 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1982-12-31 00:00:00, 1984-12-31 00:00:00]\n",
      "    maxium distance: 34.03134156425617\n",
      "    average concentration:333.89809151499327\n",
      "    eps values:[4.46541785e+00 2.23270893e+00 1.11635446e+00 4.46541785e-01\n",
      " 4.46541785e-02 4.46541785e-03 4.46541785e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:11362\n",
      "    number of samples used:11363\n",
      "    \n",
      "-0 - (4.465417852131229, 'euclidean', 11362)\n",
      "dbscan done, time=6.974282741546631 seconds | silhoutte score:all noise\n",
      "-1 - (2.2327089260656146, 'euclidean', 11362)\n",
      "dbscan done, time=3.0012688636779785 seconds | silhoutte score:all noise\n",
      "-2 - (1.1163544630328073, 'euclidean', 11362)\n",
      "dbscan done, time=2.1934423446655273 seconds | silhoutte score:all noise\n",
      "-3 - (0.4465417852131229, 'euclidean', 11362)\n",
      "dbscan done, time=1.9907212257385254 seconds | silhoutte score:all noise\n",
      "-4 - (0.04465417852131229, 'euclidean', 11362)\n",
      "dbscan done, time=1.9936740398406982 seconds | silhoutte score:all noise\n",
      "-5 - (0.004465417852131229, 'euclidean', 11362)\n",
      "dbscan done, time=2.079885721206665 seconds | silhoutte score:all noise\n",
      "-6 - (4.465417852131229e-05, 'euclidean', 11362)\n",
      "dbscan done, time=2.222059488296509 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1984-12-31 00:00:00, 1986-12-31 00:00:00]\n",
      "    maxium distance: 23.397066175570483\n",
      "    average concentration:481.08595819388233\n",
      "    eps values:[4.50267867e+00 2.25133933e+00 1.12566967e+00 4.50267867e-01\n",
      " 4.50267867e-02 4.50267867e-03 4.50267867e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:11255\n",
      "    number of samples used:11256\n",
      "    \n",
      "-0 - (4.502678667791347, 'euclidean', 11255)\n",
      "dbscan done, time=7.655142545700073 seconds | silhoutte score:all noise\n",
      "-1 - (2.2513393338956735, 'euclidean', 11255)\n",
      "dbscan done, time=2.760976552963257 seconds | silhoutte score:all noise\n",
      "-2 - (1.1256696669478368, 'euclidean', 11255)\n",
      "dbscan done, time=2.486210584640503 seconds | silhoutte score:all noise\n",
      "-3 - (0.45026786677913466, 'euclidean', 11255)\n",
      "dbscan done, time=2.4095630645751953 seconds | silhoutte score:all noise\n",
      "-4 - (0.04502678667791347, 'euclidean', 11255)\n",
      "dbscan done, time=3.5846941471099854 seconds | silhoutte score:all noise\n",
      "-5 - (0.004502678667791347, 'euclidean', 11255)\n",
      "dbscan done, time=2.9980342388153076 seconds | silhoutte score:all noise\n",
      "-6 - (4.502678667791347e-05, 'euclidean', 11255)\n",
      "dbscan done, time=2.1455163955688477 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1986-12-31 00:00:00, 1988-12-31 00:00:00]\n",
      "    maxium distance: 28.04966468818531\n",
      "    average concentration:384.7104812110369\n",
      "    eps values:[4.60402092e+00 2.30201046e+00 1.15100523e+00 4.60402092e-01\n",
      " 4.60402092e-02 4.60402092e-03 4.60402092e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:10790\n",
      "    number of samples used:10791\n",
      "    \n",
      "-0 - (4.604020920402388, 'euclidean', 10790)\n",
      "dbscan done, time=3.9849798679351807 seconds | silhoutte score:all noise\n",
      "-1 - (2.302010460201194, 'euclidean', 10790)\n",
      "dbscan done, time=2.7229042053222656 seconds | silhoutte score:all noise\n",
      "-2 - (1.151005230100597, 'euclidean', 10790)\n",
      "dbscan done, time=2.577115774154663 seconds | silhoutte score:all noise\n",
      "-3 - (0.46040209204023885, 'euclidean', 10790)\n",
      "dbscan done, time=2.258371591567993 seconds | silhoutte score:all noise\n",
      "-4 - (0.04604020920402389, 'euclidean', 10790)\n",
      "dbscan done, time=1.9616460800170898 seconds | silhoutte score:all noise\n",
      "-5 - (0.004604020920402388, 'euclidean', 10790)\n",
      "dbscan done, time=2.61971378326416 seconds | silhoutte score:all noise\n",
      "-6 - (4.604020920402389e-05, 'euclidean', 10790)\n",
      "dbscan done, time=3.0176589488983154 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1988-12-31 00:00:00, 1990-12-31 00:00:00]\n",
      "    maxium distance: 39.17448401476305\n",
      "    average concentration:280.6673853280746\n",
      "    eps values:[4.56531382e+00 2.28265691e+00 1.14132845e+00 4.56531382e-01\n",
      " 4.56531382e-02 4.56531382e-03 4.56531382e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:10994\n",
      "    number of samples used:10995\n",
      "    \n",
      "-0 - (4.565313815062814, 'euclidean', 10994)\n",
      "dbscan done, time=6.638134002685547 seconds | silhoutte score:all noise\n",
      "-1 - (2.282656907531407, 'euclidean', 10994)\n",
      "dbscan done, time=2.2174952030181885 seconds | silhoutte score:all noise\n",
      "-2 - (1.1413284537657036, 'euclidean', 10994)\n",
      "dbscan done, time=1.9219601154327393 seconds | silhoutte score:all noise\n",
      "-3 - (0.4565313815062814, 'euclidean', 10994)\n",
      "dbscan done, time=2.3657500743865967 seconds | silhoutte score:all noise\n",
      "-4 - (0.045653138150628145, 'euclidean', 10994)\n",
      "dbscan done, time=2.123070001602173 seconds | silhoutte score:all noise\n",
      "-5 - (0.004565313815062815, 'euclidean', 10994)\n",
      "dbscan done, time=1.9120264053344727 seconds | silhoutte score:all noise\n",
      "-6 - (4.565313815062814e-05, 'euclidean', 10994)\n",
      "dbscan done, time=1.9130053520202637 seconds | silhoutte score:all noise\n",
      "\n",
      "    period (1990-12-31 00:00:00, 1992-12-31 00:00:00]\n",
      "    maxium distance: 38.89574815579125\n",
      "    average concentration:305.56039062152405\n",
      "    eps values:[4.78616817e+00 2.39308408e+00 1.19654204e+00 4.78616817e-01\n",
      " 4.78616817e-02 4.78616817e-03 4.78616817e-05]\n",
      "    used metrics:['euclidean']\n",
      "    number of minimum samples:11884\n",
      "    number of samples used:11885\n",
      "    \n",
      "-0 - (4.786168165221251, 'euclidean', 11884)\n",
      "dbscan done, time=4.935248613357544 seconds | silhoutte score:all noise\n",
      "-1 - (2.3930840826106254, 'euclidean', 11884)\n",
      "dbscan done, time=2.9463653564453125 seconds | silhoutte score:all noise\n",
      "-2 - (1.1965420413053127, 'euclidean', 11884)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124mperiod \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#normalization is done for each group\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_dbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_pts_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecade_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mk\n\u001b[1;32m     47\u001b[0m group_results\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([group_results,result])\n",
      "File \u001b[0;32m~/DataMiningProject/src/task3_clustering/utils.py:28\u001b[0m, in \u001b[0;36mrun_dbscan\u001b[0;34m(min_pts_values, eps_values, metric, clustering_data, precomputed_distances)\u001b[0m\n\u001b[1;32m     21\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(eps,metric,min_pts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m labels\u001b[38;5;241m=\u001b[39m\u001b[43mDBSCAN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_pts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m---> 28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m end\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m silhouette_score_val\u001b[38;5;241m=\u001b[39msilhouette_score(clustering_data,labels) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (labels\u001b[38;5;241m==\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall core\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall noise\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/cluster/_dbscan.py:474\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/cluster/_dbscan.py:422\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    420\u001b[0m neighbors_model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:1199\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors\u001b[0;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     results \u001b[38;5;241m=\u001b[39m RadiusNeighbors\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m   1186\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   1187\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         sort_results\u001b[38;5;241m=\u001b[39msort_results,\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m   1198\u001b[0m ):\n\u001b[0;32m-> 1199\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43m_radius_neighbors_from_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;66;03m# Joblib-based backend, which is used when user-defined callable\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;66;03m# are passed for metric.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# for efficiency, use squared euclidean distances\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/neighbors/_base.py:377\u001b[0m, in \u001b[0;36m_radius_neighbors_from_graph\u001b[0;34m(graph, radius, return_distance)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[1;32m    376\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcompress(mask, graph\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 377\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     indptr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mcumsum(mask)))[graph\u001b[38;5;241m.\u001b[39mindptr]\n\u001b[1;32m    380\u001b[0m indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39mno_filter_needed)\n",
      "File \u001b[0;32m~/anaconda3/envs/dmproj/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2028\u001b[0m, in \u001b[0;36m_compress_dispatcher\u001b[0;34m(condition, a, axis, out)\u001b[0m\n\u001b[1;32m   2024\u001b[0m         result \u001b[38;5;241m=\u001b[39m asarray(a)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 2028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compress_dispatcher\u001b[39m(condition, a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, a, out)\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_compress_dispatcher)\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(condition, a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "# useful for reference\n",
    "db_scan_mapping={\n",
    "    -1:'noisy',\n",
    "    0:'border',\n",
    "    1:'core'\n",
    "\n",
    "}\n",
    "\n",
    "std_scaler=StandardScaler()\n",
    "\n",
    "group_results=pd.DataFrame()\n",
    "\n",
    "\n",
    "for k,decade_data in normalized_decade_groups.items():\n",
    "    #NOTE: this might have to be revisited for it's just to try if everyting works\n",
    "    dimension=decade_data.shape[0]\n",
    "    min_pts=int(dimension-1)\n",
    "    #using the method seen at laboratory to select initial values\n",
    "    #print(decade_data.drop(columns=\"decade\").info())\n",
    "    maximum_distance = abs(decade_data.max() - decade_data.min()).sum().item()\n",
    "    average_concentration = dimension / maximum_distance\n",
    "    #use diferent scales for eps values\n",
    "    # during the tests a lot of low values where not taken into consideration\n",
    "    eps_values=initial_eps[k] * np.array([500,250,100,50,10, 5, 2.5, 1, 0.1, 0.01, 0.0001])\n",
    "    #try various metrics\n",
    "    metrics=['euclidean']\n",
    "\n",
    "    min_pts_values=[min_pts]\n",
    "    print(\n",
    "    f\"\"\"\n",
    "    period {k}\n",
    "    maxium distance: {maximum_distance}\n",
    "    average concentration:{average_concentration}\n",
    "    eps values:{eps_values}\n",
    "    used metrics:{metrics}\n",
    "    number of minimum samples:{min_pts}\n",
    "    number of samples used:{decade_data.shape[0]}\n",
    "    \"\"\"\n",
    "    )\n",
    "    #normalization is done for each group\n",
    "    result=utils.run_dbscan(min_pts_values,eps_values,metrics,decade_data)\n",
    "    result[\"group\"]=k\n",
    "    group_results=pd.concat([group_results,result])\n",
    "group_results.reset_index().sort_values(by='silhoutte_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx=group_results['silhoutte_score'].argmax()\n",
    "best_params=group_results.iloc[best_idx]\n",
    "best_eps=best_params['eps']\n",
    "best_metric=best_params['metric']\n",
    "\n",
    "best_dbscan=DBSCAN(eps=best_eps,metric=best_metric,min_samples=min_pts).fit(reduction_data)\n",
    "\n",
    "labels=best_dbscan.labels_\n",
    "\n",
    "statistics=np.unique(best_dbscan.labels_,return_counts=True)\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "results:{best_params}\n",
    "statistics:\n",
    "    raw counts:{statistics}\n",
    "    percentags:{statistics/np.sum(statistics)}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- segmentazione sui migliori o per anni\n",
    "- rifare per altri in caso\n",
    "- plot BSS e SSE sÃ¬\n",
    "- aggregazione gare sÃ¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
