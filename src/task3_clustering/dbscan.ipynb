{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "This notebook uses DBSCAN as a clustering density based approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>length</th>\n",
       "      <th>climb_total</th>\n",
       "      <th>profile</th>\n",
       "      <th>startlist_quality</th>\n",
       "      <th>position</th>\n",
       "      <th>cyclist_age</th>\n",
       "      <th>delta</th>\n",
       "      <th>climbing_efficiency</th>\n",
       "      <th>competitive_age</th>\n",
       "      <th>...</th>\n",
       "      <th>convenience_score</th>\n",
       "      <th>difficulty_score</th>\n",
       "      <th>performance_index</th>\n",
       "      <th>gain_ratio</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>easy</th>\n",
       "      <th>hard</th>\n",
       "      <th>moderate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "      <td>589865.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>89.221635</td>\n",
       "      <td>166776.180584</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>2.673627</td>\n",
       "      <td>1101.161178</td>\n",
       "      <td>74.219491</td>\n",
       "      <td>28.486208</td>\n",
       "      <td>418.292794</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>28.485343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035668</td>\n",
       "      <td>1.244299</td>\n",
       "      <td>0.328403</td>\n",
       "      <td>179.786890</td>\n",
       "      <td>15.090092</td>\n",
       "      <td>5.900314</td>\n",
       "      <td>2006.166425</td>\n",
       "      <td>0.316364</td>\n",
       "      <td>0.111405</td>\n",
       "      <td>0.572231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.413315</td>\n",
       "      <td>64545.605664</td>\n",
       "      <td>1191.967186</td>\n",
       "      <td>1.349810</td>\n",
       "      <td>380.586928</td>\n",
       "      <td>48.404023</td>\n",
       "      <td>3.855262</td>\n",
       "      <td>842.961596</td>\n",
       "      <td>0.062654</td>\n",
       "      <td>3.856015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058427</td>\n",
       "      <td>0.549808</td>\n",
       "      <td>0.172467</td>\n",
       "      <td>1027.920690</td>\n",
       "      <td>7.948271</td>\n",
       "      <td>1.981774</td>\n",
       "      <td>11.477521</td>\n",
       "      <td>0.465057</td>\n",
       "      <td>0.314633</td>\n",
       "      <td>0.494756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>-6906.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.834738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>152500.000000</td>\n",
       "      <td>1666.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>844.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.874988</td>\n",
       "      <td>0.192755</td>\n",
       "      <td>40.918990</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>178200.000000</td>\n",
       "      <td>2330.469215</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>988.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>1.262876</td>\n",
       "      <td>0.310048</td>\n",
       "      <td>61.405397</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>203500.000000</td>\n",
       "      <td>2863.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>624.000000</td>\n",
       "      <td>0.018083</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042161</td>\n",
       "      <td>1.582046</td>\n",
       "      <td>0.446206</td>\n",
       "      <td>116.276626</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>338000.000000</td>\n",
       "      <td>6974.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2047.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>61547.000000</td>\n",
       "      <td>1.370864</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.752226</td>\n",
       "      <td>0.990431</td>\n",
       "      <td>50233.876744</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              points         length    climb_total        profile  \\\n",
       "count  589865.000000  589865.000000  589865.000000  589865.000000   \n",
       "mean       89.221635  166776.180584    2330.469215       2.673627   \n",
       "std        54.413315   64545.605664    1191.967186       1.349810   \n",
       "min        18.000000    1000.000000       2.000000       1.000000   \n",
       "25%        50.000000  152500.000000    1666.000000       2.000000   \n",
       "50%        80.000000  178200.000000    2330.469215       3.000000   \n",
       "75%       100.000000  203500.000000    2863.000000       3.000000   \n",
       "max       350.000000  338000.000000    6974.000000       5.000000   \n",
       "\n",
       "       startlist_quality       position    cyclist_age          delta  \\\n",
       "count      589865.000000  589865.000000  589865.000000  589865.000000   \n",
       "mean         1101.161178      74.219491      28.486208     418.292794   \n",
       "std           380.586928      48.404023       3.855262     842.961596   \n",
       "min           115.000000       0.000000      13.000000   -6906.000000   \n",
       "25%           844.000000      32.000000      26.000000      10.000000   \n",
       "50%           988.000000      70.000000      28.000000     156.000000   \n",
       "75%          1309.000000     112.000000      31.000000     624.000000   \n",
       "max          2047.000000     209.000000      56.000000   61547.000000   \n",
       "\n",
       "       climbing_efficiency  competitive_age  ...  convenience_score  \\\n",
       "count        589865.000000    589865.000000  ...      589865.000000   \n",
       "mean              0.023028        28.485343  ...           0.035668   \n",
       "std               0.062654         3.856015  ...           0.058427   \n",
       "min               0.000069        13.000000  ...           0.000000   \n",
       "25%               0.009397        26.000000  ...           0.004432   \n",
       "50%               0.013032        28.000000  ...           0.013905   \n",
       "75%               0.018083        31.000000  ...           0.042161   \n",
       "max               1.370864        56.000000  ...           1.000000   \n",
       "\n",
       "       difficulty_score  performance_index     gain_ratio            day  \\\n",
       "count     589865.000000      589865.000000  589865.000000  589865.000000   \n",
       "mean           1.244299           0.328403     179.786890      15.090092   \n",
       "std            0.549808           0.172467    1027.920690       7.948271   \n",
       "min            0.001593           0.000000      11.834738       1.000000   \n",
       "25%            0.874988           0.192755      40.918990       9.000000   \n",
       "50%            1.262876           0.310048      61.405397      14.000000   \n",
       "75%            1.582046           0.446206     116.276626      22.000000   \n",
       "max            2.752226           0.990431   50233.876744      31.000000   \n",
       "\n",
       "               month           year           easy           hard  \\\n",
       "count  589865.000000  589865.000000  589865.000000  589865.000000   \n",
       "mean        5.900314    2006.166425       0.316364       0.111405   \n",
       "std         1.981774      11.477521       0.465057       0.314633   \n",
       "min         2.000000    1970.000000       0.000000       0.000000   \n",
       "25%         4.000000    1999.000000       0.000000       0.000000   \n",
       "50%         6.000000    2008.000000       0.000000       0.000000   \n",
       "75%         7.000000    2015.000000       1.000000       0.000000   \n",
       "max        11.000000    2023.000000       1.000000       1.000000   \n",
       "\n",
       "            moderate  \n",
       "count  589865.000000  \n",
       "mean        0.572231  \n",
       "std         0.494756  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "RACES_PATH=path.join(\"..\",\"dataset\",\"engineered_races.csv\")\n",
    "\n",
    "races_df=pd.read_csv(RACES_PATH)\n",
    "\n",
    "#print(races_df.describe())\n",
    "\n",
    "cols=list(races_df.columns)\n",
    "#too noisy to use since it cuts away too much information\n",
    "cols.remove(\"is_tarmac\")\n",
    "# not really relevant unless we want to find a usefull correlation\n",
    "cols.remove(\"stage\")\n",
    "# useless unless we care about grouping together performances of cyclists but overall might be noisy\n",
    "cols.remove(\"std_name\")\n",
    "# not really usefull unless we care about teams performances\n",
    "# also the way it was filled is difficult to make it useful it might be very noisy\n",
    "cols.remove(\"cyclist_team\")\n",
    "#same as above\n",
    "cols.remove(\"cyclist\")\n",
    "\n",
    "\n",
    "clustering_data=races_df[cols].copy()\n",
    "#convert to timestamp(units are useless since it's getting normalized)\n",
    "clustering_data['date']=pd.to_datetime(clustering_data['date'])\n",
    "clustering_data['day']=clustering_data['date'].dt.day\n",
    "clustering_data['month']=clustering_data['date'].dt.month\n",
    "clustering_data['year']=clustering_data['date'].dt.year\n",
    "\n",
    "#one hot encoding difficulty\n",
    "ohe_diff_lvl=pd.get_dummies(races_df['difficulty_level']).astype(float)\n",
    "\n",
    "#clustering_data\n",
    "dec_cut=pd.date_range(\n",
    "    start=clustering_data['date'].min(),\n",
    "    end=clustering_data['date'].max(),\n",
    "    freq='10YE'\n",
    ")\n",
    "dec_cuts=pd.cut(\n",
    "    clustering_data['date'],\n",
    "    bins=dec_cut\n",
    ")\n",
    "\n",
    "clustering_data=clustering_data.drop(columns=[\"date\",\"difficulty_level\"])\n",
    "clustering_data['decade']=dec_cuts\n",
    "clustering_data[ohe_diff_lvl.columns]=ohe_diff_lvl\n",
    "\n",
    "clustering_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before starting, for reasons of time we could,for this delivery, do the clustering on the full dataset so for now we decided to employ some sort of data reduction as to make it feasible to run such an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering organization\n",
    "\n",
    "For reasons of time using DBSCAN on the whole dataset is not feasible, a second approach would be to try and a segmentation, for this part we wanted to employ a clusterization that is time based and analyses clusters across decades and see what we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10147/1662862600.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  dec_groups=reduction_data.groupby('decade')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">points</th>\n",
       "      <th colspan=\"2\" halign=\"left\">length</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hard</th>\n",
       "      <th colspan=\"8\" halign=\"left\">moderate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1970-12-31 00:00:00, 1980-12-31 00:00:00]</th>\n",
       "      <td>11163.0</td>\n",
       "      <td>116.966317</td>\n",
       "      <td>70.661337</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>11163.0</td>\n",
       "      <td>169754.670787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11163.0</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.493516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1980-12-31 00:00:00, 1990-12-31 00:00:00]</th>\n",
       "      <td>43512.0</td>\n",
       "      <td>111.688633</td>\n",
       "      <td>57.693682</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>43512.0</td>\n",
       "      <td>172376.040173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43512.0</td>\n",
       "      <td>0.497012</td>\n",
       "      <td>0.499997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1990-12-31 00:00:00, 2000-12-31 00:00:00]</th>\n",
       "      <td>79122.0</td>\n",
       "      <td>95.979991</td>\n",
       "      <td>55.074528</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>79122.0</td>\n",
       "      <td>173879.013422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79122.0</td>\n",
       "      <td>0.575124</td>\n",
       "      <td>0.494327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000-12-31 00:00:00, 2010-12-31 00:00:00]</th>\n",
       "      <td>142380.0</td>\n",
       "      <td>81.894437</td>\n",
       "      <td>50.323952</td>\n",
       "      <td>18.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>142380.0</td>\n",
       "      <td>162432.915438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142380.0</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.493812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2010-12-31 00:00:00, 2020-12-31 00:00:00]</th>\n",
       "      <td>150837.0</td>\n",
       "      <td>85.446305</td>\n",
       "      <td>52.293645</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>150837.0</td>\n",
       "      <td>166492.978845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150837.0</td>\n",
       "      <td>0.577259</td>\n",
       "      <td>0.493997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points                         \\\n",
       "                                               count        mean        std   \n",
       "decade                                                                        \n",
       "(1970-12-31 00:00:00, 1980-12-31 00:00:00]   11163.0  116.966317  70.661337   \n",
       "(1980-12-31 00:00:00, 1990-12-31 00:00:00]   43512.0  111.688633  57.693682   \n",
       "(1990-12-31 00:00:00, 2000-12-31 00:00:00]   79122.0   95.979991  55.074528   \n",
       "(2000-12-31 00:00:00, 2010-12-31 00:00:00]  142380.0   81.894437  50.323952   \n",
       "(2010-12-31 00:00:00, 2020-12-31 00:00:00]  150837.0   85.446305  52.293645   \n",
       "\n",
       "                                                                              \\\n",
       "                                             min    25%    50%    75%    max   \n",
       "decade                                                                         \n",
       "(1970-12-31 00:00:00, 1980-12-31 00:00:00]  50.0   80.0  100.0  100.0  350.0   \n",
       "(1980-12-31 00:00:00, 1990-12-31 00:00:00]  50.0  100.0  100.0  100.0  350.0   \n",
       "(1990-12-31 00:00:00, 2000-12-31 00:00:00]  50.0   80.0   80.0  100.0  350.0   \n",
       "(2000-12-31 00:00:00, 2010-12-31 00:00:00]  18.0   50.0   80.0   80.0  350.0   \n",
       "(2010-12-31 00:00:00, 2020-12-31 00:00:00]  50.0   50.0   80.0  100.0  350.0   \n",
       "\n",
       "                                              length                 ... hard  \\\n",
       "                                               count           mean  ...  75%   \n",
       "decade                                                               ...        \n",
       "(1970-12-31 00:00:00, 1980-12-31 00:00:00]   11163.0  169754.670787  ...  0.0   \n",
       "(1980-12-31 00:00:00, 1990-12-31 00:00:00]   43512.0  172376.040173  ...  0.0   \n",
       "(1990-12-31 00:00:00, 2000-12-31 00:00:00]   79122.0  173879.013422  ...  0.0   \n",
       "(2000-12-31 00:00:00, 2010-12-31 00:00:00]  142380.0  162432.915438  ...  0.0   \n",
       "(2010-12-31 00:00:00, 2020-12-31 00:00:00]  150837.0  166492.978845  ...  0.0   \n",
       "\n",
       "                                                 moderate                      \\\n",
       "                                            max     count      mean       std   \n",
       "decade                                                                          \n",
       "(1970-12-31 00:00:00, 1980-12-31 00:00:00]  1.0   11163.0  0.580400  0.493516   \n",
       "(1980-12-31 00:00:00, 1990-12-31 00:00:00]  1.0   43512.0  0.497012  0.499997   \n",
       "(1990-12-31 00:00:00, 2000-12-31 00:00:00]  1.0   79122.0  0.575124  0.494327   \n",
       "(2000-12-31 00:00:00, 2010-12-31 00:00:00]  1.0  142380.0  0.578431  0.493812   \n",
       "(2010-12-31 00:00:00, 2020-12-31 00:00:00]  1.0  150837.0  0.577259  0.493997   \n",
       "\n",
       "                                                                     \n",
       "                                            min  25%  50%  75%  max  \n",
       "decade                                                               \n",
       "(1970-12-31 00:00:00, 1980-12-31 00:00:00]  0.0  0.0  1.0  1.0  1.0  \n",
       "(1980-12-31 00:00:00, 1990-12-31 00:00:00]  0.0  0.0  0.0  1.0  1.0  \n",
       "(1990-12-31 00:00:00, 2000-12-31 00:00:00]  0.0  0.0  1.0  1.0  1.0  \n",
       "(2000-12-31 00:00:00, 2010-12-31 00:00:00]  0.0  0.0  1.0  1.0  1.0  \n",
       "(2010-12-31 00:00:00, 2020-12-31 00:00:00]  0.0  0.0  1.0  1.0  1.0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "clustering_data=clustering_data.drop_duplicates()\n",
    "\n",
    "num_samples=clustering_data.shape[0]\n",
    "\n",
    "reduction_percent=0.8\n",
    "\n",
    "reduction_num_samples=int(np.ceil(reduction_percent*num_samples))\n",
    "\n",
    "RANDOM_SEED=42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "reduction_idx=np.random.choice(range(len(clustering_data)),reduction_num_samples,replace=False)\n",
    "\n",
    "reduction_data=clustering_data.iloc[reduction_idx]\n",
    "\n",
    "\n",
    "dec_groups=reduction_data.groupby('decade')\n",
    "\n",
    "dec_groups.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes are due before starting, first the eps are difficulty to setup for now a good strategy would be to take inspiration using the first paper the introduced the algorithm, which you can find [here](https://dl.acm.org/doi/10.5555/3001460.3001507), and use the distance from the k-th NN varying K until we find a good eps value for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    group no.(1970-12-31 00:00:00, 1980-12-31 00:00:00]\n",
      "\n",
      "    average concentration:5.095399183535452e-05\n",
      "\n",
      "    eps values:[5.09539918e-04 2.54769959e-04 1.27384980e-04 5.09539918e-05\n",
      " 5.09539918e-06 5.09539918e-07 5.09539918e-09]\n",
      "\n",
      "    used metrics:['euclidean', 'cosine', 'l1']\n",
      "\n",
      "    number of minimum samples:5581\n",
      "\n",
      "    number of samples used:11163\n",
      "    \n",
      "starting 0\n",
      " {'index': 0, 'eps': 0.0005095399183535452, 'metric': 'euclidean', 'min_samples': 5581}\n",
      "(array([-1]), array([11163]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124mgroup no.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#normalization is done for each group\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_dbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_pts_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecade_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecade\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mk\n\u001b[1;32m     46\u001b[0m group_results\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([group_results,result])\n",
      "File \u001b[0;32m~/DataMiningProject/src/task3_clustering/utils.py:31\u001b[0m, in \u001b[0;36mrun_dbscan\u001b[0;34m(min_pts_values, eps_values, metric, clustering_data)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(labels,return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#NOTE: the noisy labels are NOT taken into account \u001b[39;00m\n\u001b[1;32m     26\u001b[0m new_conf\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m:idx,\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m:eps,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m:metric,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples\u001b[39m\u001b[38;5;124m'\u001b[39m:min_pts,\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhoutte_score\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustering_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     }\n\u001b[1;32m     33\u001b[0m results\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([results,new_conf])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:141\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:299\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    297\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m    298\u001b[0m label_freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(labels)\n\u001b[0;32m--> 299\u001b[0m \u001b[43mcheck_number_of_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    302\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    303\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[1;32m    304\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dmproj/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:38\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Number of samples.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n_labels \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;241m%\u001b[39m n_labels\n\u001b[1;32m     41\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# useful for reference\n",
    "db_scan_mapping={\n",
    "    -1:'noisy',\n",
    "    0:'border',\n",
    "    1:'core'\n",
    "\n",
    "}\n",
    "\n",
    "std_scaler=StandardScaler()\n",
    "\n",
    "group_results=pd.DataFrame()\n",
    "\n",
    "for k,decade_data in dec_groups:\n",
    "    #NOTE: this might have to be revisited for it's just to try if everyting works\n",
    "    dimension=decade_data.shape[0]\n",
    "    min_pts=int(dimension/2)\n",
    "    #using the method seen at laboratory to select initial values\n",
    "    #print(decade_data.drop(columns=\"decade\").info())\n",
    "    maximum_distance = abs(decade_data.drop(columns=\"decade\").max() - decade_data.drop(columns=\"decade\").min()).sum().item()\n",
    "    average_concentration = dimension / maximum_distance\n",
    "    #use diferent scales for eps values\n",
    "    eps_values=average_concentration * np.array([10, 5, 2.5, 1, 0.1, 0.01, 0.0001])\n",
    "    #try various metrics\n",
    "    metrics=['euclidean','cosine','l1']\n",
    "\n",
    "    min_pts_values=[min_pts]\n",
    "    print(\n",
    "    f\"\"\"\n",
    "    group no.{k}\n",
    "\n",
    "    average concentration:{average_concentration}\n",
    "\n",
    "    eps values:{eps_values}\n",
    "\n",
    "    used metrics:{metrics}\n",
    "\n",
    "    number of minimum samples:{min_pts}\n",
    "\n",
    "    number of samples used:{decade_data.shape[0]}\n",
    "    \"\"\"\n",
    "    )\n",
    "    #normalization is done for each group\n",
    "    result=utils.run_dbscan(min_pts_values,eps_values,metrics,std_scaler.fit_transform(decade_data.drop(columns=\"decade\")))\n",
    "    result[\"group\"]=k\n",
    "\n",
    "    group_results=pd.concat([group_results,result])\n",
    "    print(result.sort_values(by='silhoutte_score'))\n",
    "\n",
    "group_results.sort_values(by='silhoutte_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx=results['silhoutte_score'].argmax()\n",
    "best_params=results.iloc[best_idx]\n",
    "best_eps=best_params['eps']\n",
    "best_metric=best_params['metric']\n",
    "\n",
    "best_dbscan=DBSCAN(eps=best_eps,metric=best_metric,min_samples=min_pts).fit(reduction_data)\n",
    "\n",
    "labels=best_dbscan.labels_\n",
    "\n",
    "statistics=np.unique(best_dbscan.labels_,return_counts=True)\n",
    "\n",
    "print(f\"\"\"\n",
    "results:{best_params}\n",
    "statistics:\n",
    "    raw counts:{statistics}\n",
    "    percentags:{statistics/np.sum(statistics)}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- segmentazione sui migliori o per anni\n",
    "- rifare per altri in caso\n",
    "- plot BSS e SSE sÃ¬\n",
    "- aggregazione gare sÃ¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
